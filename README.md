# Итоговый проект

### Описание
Выполнен вариант ETL процесса обработки данных из нескольких источников с хранилищем данных, подготовлена витрина в соответствии с поставленной задачей. DWH-хранилище поднято в БД **Vertica**. Выгрузка из БД **PostgreSQL** в DWH и обновление витрины автоматизированы с помощью **Airflow**. Метрики визуализированы в **Metabase**.

### Структура репозитория
Витрина называется `global_metrics`
Схема в которой лежит витрина - `STV2023081241__DWH`

Внутри `src` расположены папки:
- `/src/dags` - DAG файлы, которые поставляет данные из источника в хранилище.
   * `0_make_tables.py` - DAG для создания первичной структуры таблиц в БД.
   * `1_data_import.py` - DAG для поставки данных из источника Postgresql в Vertica.
   * `2_datamart_update.py` - DAG формирования витрины в Vertica.
- `/src/sql` - SQL-запросы формирования таблиц в `STAGING`- и `DWH`-слоях, а также скрипт подготовки данных для итоговой витрины.
- `/src/img` - скриншот реализованного в Metabase дашборда.

### Подключение
Переменные Airflow для подключения к БД PostgeSQL и БД Vertica находятся в файле `variables.json`
